{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# library importing"
      ],
      "metadata": {
        "id": "t6GB-fUCNIC-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UTChxDW4Mx9h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "import dis\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "import platform\n",
        "import statistics\n",
        "import subprocess\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import glob\n",
        "import copy\n",
        "import inspect\n",
        "import importlib.util\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, Callable, Any, Optional, List\n",
        "from types import MappingProxyType, FunctionType\n",
        "from tqdm.auto import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# constants"
      ],
      "metadata": {
        "id": "hfAh1VlUmwW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_zip_file(zip_filename, target_directory):\n",
        "    \"\"\"\n",
        "    Unzips a zip file to the specified directory and deletes the archive\n",
        "    \"\"\"\n",
        "    zip_path = f'./{zip_filename}'\n",
        "\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f\"Error: file {zip_path} not found!\")\n",
        "        return False\n",
        "\n",
        "    os.makedirs(target_directory, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            print(f\"Extracting {zip_filename}...\")\n",
        "\n",
        "            file_list = zip_ref.namelist()\n",
        "            print(f\"Files found in archive: {len(file_list)}\")\n",
        "\n",
        "            zip_ref.extractall(target_directory)\n",
        "\n",
        "            for file_name in file_list:\n",
        "                extracted_path = os.path.join(target_directory, file_name)\n",
        "                if os.path.isfile(extracted_path):\n",
        "                    file_size = os.path.getsize(extracted_path)\n",
        "                    print(f\"Exctracted: {file_name} ({file_size} byte)\")\n",
        "\n",
        "        os.remove(zip_path)\n",
        "        print(f\"Zip-file {zip_filename} has been deleted\")\n",
        "\n",
        "        print(f\"Operation completed successfully! Files saved in: {target_directory}\")\n",
        "        return True\n",
        "\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"Erroe: {zip_filename} is corrupted or is not a zip file\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"Error while processing: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# shutil.rmtree('./' + 'ds_tools')\n",
        "# shutil.rmtree('./' + 'cost_models')"
      ],
      "metadata": {
        "id": "6Vi0dsEblPYC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "DEFAULT_EXAMPLES_PATH = \"./examples\"\n",
        "# SAVING_FLAG = True\n",
        "DEFAULT_COST_MODEL_PATH = \"./cost_models\"\n",
        "\n",
        "os.makedirs(DEFAULT_COST_MODEL_PATH[2:], exist_ok=True)\n",
        "os.makedirs(DEFAULT_EXAMPLES_PATH[2:], exist_ok=True)\n",
        "zip_filename_models = \"cost_models.zip\"\n",
        "zip_filename_examples = \"examples.zip\"\n",
        "\n",
        "process_zip_file(zip_filename_models, DEFAULT_COST_MODEL_PATH)\n",
        "process_zip_file(zip_filename_examples, DEFAULT_EXAMPLES_PATH)\n",
        "\n",
        "import utils as u\n",
        "import benchmark_algorithms as ba\n",
        "import composite_score_calculator as csc\n",
        "from instruction_cost_model import InstructionCostModel\n",
        "from enhanced_cost_analyzer import EnhancedCostAnalyzer"
      ],
      "metadata": {
        "id": "FDiokNQQmzhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://example.com/file.zip"
      ],
      "metadata": {
        "id": "iZejS-MuPfIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# needed functions and classes"
      ],
      "metadata": {
        "id": "xIXLbcAjNw3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def main() -> None:\n",
        "#     \"\"\"Enhanced main execution function with composite scoring examples.\"\"\"\n",
        "#     # Architecture detection and analyzer initialization\n",
        "#     detected_arch = platform.machine().lower()\n",
        "#     print(f\"[Init] Detected architecture: {detected_arch}\")\n",
        "\n",
        "#     # Initialize enhanced analyzer with profile-based weights\n",
        "#     print(f\"[Config] Available profiles: {list(PROFILE_WEIGHTS.keys())}\")\n",
        "\n",
        "#     # Example: Use RESEARCH profile for academic/research scenarios\n",
        "#     analyzer = EnhancedCostAnalyzer(arch=detected_arch, profile=\"RESEARCH\")\n",
        "\n",
        "#     # Print current profile info\n",
        "#     profile_info = analyzer.composite_calculator.get_profile_info()\n",
        "#     print(f\"[Config] Using profile: {profile_info['profile']}\")\n",
        "#     print(f\"[Config] Profile description: {profile_info['description']}\")\n",
        "#     print(f\"[Config] Weights: {profile_info['weights']}\")\n",
        "\n",
        "#     # Single function analysis with composite scoring\n",
        "#     print(\"\\n\" + \"=\"*60)\n",
        "#     print(\"ENHANCED ALGORITHM ANALYSIS WITH COMPOSITE SCORING\")\n",
        "#     print(\"=\"*60)\n",
        "\n",
        "#     result_linear = analyzer.analyze_function(algorithm_linear)\n",
        "#     result_constant = analyzer.analyze_function(algorithm_constant)\n",
        "\n",
        "#     print(f\"\\n[Analysis] Linear Algorithm Assessment:\")\n",
        "#     for key, value in result_linear.items():\n",
        "#         if isinstance(value, (int, float)):\n",
        "#             print(f\"  {key}: {value:.6f}\")\n",
        "#         else:\n",
        "#             print(f\"  {key}: {value}\")\n",
        "\n",
        "#     print(f\"\\n[Analysis] Constant Algorithm Assessment:\")\n",
        "#     for key, value in result_constant.items():\n",
        "#         if isinstance(value, (int, float)):\n",
        "#             print(f\"  {key}: {value:.6f}\")\n",
        "#         else:\n",
        "#             print(f\"  {key}: {value}\")\n",
        "\n",
        "#     # Detailed function comparison\n",
        "#     print(f\"\\n[Comparison] Detailed Algorithm Comparison:\")\n",
        "#     comparison = analyzer.compare_functions(algorithm_linear, algorithm_constant)\n",
        "\n",
        "#     print(f\"  Original Algorithm:\")\n",
        "#     print(f\"    Composite Score: {comparison['old_metrics']['COMPOSITE_SCORE']:.2f} ({comparison['old_metrics']['SCORE_GRADE']})\")\n",
        "#     print(f\"    Efficiency Rating: {comparison['old_metrics']['EFFICIENCY_RATING']}\")\n",
        "\n",
        "#     print(f\"  Optimized Algorithm:\")\n",
        "#     print(f\"    Composite Score: {comparison['new_metrics']['COMPOSITE_SCORE']:.2f} ({comparison['new_metrics']['SCORE_GRADE']})\")\n",
        "#     print(f\"    Efficiency Rating: {comparison['new_metrics']['EFFICIENCY_RATING']}\")\n",
        "\n",
        "#     print(f\"  Improvement Analysis:\")\n",
        "#     print(f\"    Score Improvement: {comparison['comparison']['COMPOSITE_SCORE_diff']:.2f} points\")\n",
        "#     print(f\"    Better Algorithm: {'Yes' if comparison['comparison']['improvement'] else 'No'}\")\n",
        "\n",
        "#     for metric in [\"CU\", \"EU\", \"CO2\", \"$\"]:\n",
        "#         change = comparison['comparison'][f'{metric}_percent_change']\n",
        "#         print(f\"    {metric} Change: {change:.1f}%\")\n",
        "\n",
        "#     # Benchmark suite analysis\n",
        "#     print(f\"\\n[Benchmark] Running Algorithm Suite Analysis:\")\n",
        "\n",
        "#     algorithms_suite = [\n",
        "#         (\"Linear_O(n)\", algorithm_linear),\n",
        "#         (\"Constant_O(1)\", algorithm_constant),\n",
        "#         (\"Quadratic_O(nÂ²)\", algorithm_quadratic),\n",
        "#         (\"Recursive_Fib\", algorithm_recursive)\n",
        "#     ]\n",
        "\n",
        "#     benchmark_results = analyzer.benchmark_suite(algorithms_suite)\n",
        "\n",
        "#     print(f\"  Best Algorithm: {benchmark_results['statistics']['best_algorithm']}\")\n",
        "#     print(f\"  Worst Algorithm: {benchmark_results['statistics']['worst_algorithm']}\")\n",
        "#     print(f\"  Average Composite Score: {benchmark_results['statistics']['average_composite_score']:.2f}\")\n",
        "#     print(f\"  Score Standard Deviation: {benchmark_results['statistics']['composite_score_std']:.2f}\")\n",
        "#     print(f\"  Performance Range: {benchmark_results['statistics']['score_range']:.2f} points\")\n",
        "\n",
        "#     # Updated reference values after benchmarking\n",
        "#     print(f\"\\n[Calibration] Updated Reference Values:\")\n",
        "#     for metric, refs in benchmark_results['updated_references'].items():\n",
        "#         print(f\"  {metric}: min={refs['min']:.6f}, max={refs['max']:.6f}, typical={refs['typical']:.6f}\")\n",
        "\n",
        "#     # Profile comparison example\n",
        "#     print(f\"\\n[Profile Comparison] Testing different profiles:\")\n",
        "#     profiles_to_test = [\"HPC\", \"MOBILE\", \"COMMERCIAL\"]\n",
        "#     for test_profile in profiles_to_test:\n",
        "#         test_analyzer = EnhancedCostAnalyzer(arch=detected_arch, profile=test_profile)\n",
        "#         test_result = test_analyzer.analyze_function(algorithm_constant)\n",
        "#         print(f\"  {test_profile}: Composite Score = {test_result['COMPOSITE_SCORE']:.2f} ({test_result['SCORE_GRADE']})\")\n",
        "\n",
        "#     # Analyze external files if available\n",
        "#     llvm_path = \"examples/sample.ll\"\n",
        "#     if os.path.exists(llvm_path):\n",
        "#         llvm_result = analyzer.analyze_llvm_ir(llvm_path)\n",
        "#         print(f\"\\n[LLVM] Assessment of '{llvm_path}':\")\n",
        "#         print(f\"  Composite Score: {llvm_result['COMPOSITE_SCORE']:.2f} ({llvm_result['SCORE_GRADE']})\")\n",
        "\n",
        "#     ptx_path = \"examples/sample.ptx\"\n",
        "#     if os.path.exists(ptx_path):\n",
        "#         ptx_result = analyzer.analyze_ptx(ptx_path)\n",
        "#         print(f\"\\n[PTX] Assessment of '{ptx_path}':\")\n",
        "#         print(f\"  Composite Score: {ptx_result['COMPOSITE_SCORE']:.2f} ({ptx_result['SCORE_GRADE']})\")\n",
        "\n",
        "#     # Environmental impact analysis\n",
        "#     carbon_intensity = analyzer.fetch_carbon_intensity()\n",
        "#     print(f\"\\n[Environment] Current carbon intensity: {carbon_intensity:.6f} kgCO2/kWh\")\n",
        "\n",
        "#     # Save enhanced reports\n",
        "#     report_dir = \"enhanced_reports\"\n",
        "#     os.makedirs(report_dir, exist_ok=True)\n",
        "\n",
        "#     # Save individual results\n",
        "#     save_enhanced_csv(result_linear, \"linear_algorithm_enhanced.csv\", report_dir)\n",
        "#     save_enhanced_csv(result_constant, \"constant_algorithm_enhanced.csv\", report_dir)\n",
        "\n",
        "#     # Save comparison results\n",
        "#     with open(os.path.join(report_dir, \"detailed_comparison.json\"), \"w\", encoding='utf-8') as f:\n",
        "#         json.dump(comparison, f, indent=4)\n",
        "\n",
        "#     # Save benchmark results\n",
        "#     with open(os.path.join(report_dir, \"benchmark_suite_results.json\"), \"w\", encoding='utf-8') as f:\n",
        "#         json.dump(benchmark_results, f, indent=4)\n",
        "\n",
        "#     # Create enhanced visualizations\n",
        "#     create_enhanced_comparison_chart(\n",
        "#         result_linear, result_constant, report_dir,\n",
        "#         names=(\"Linear O(n)\", \"Constant O(1)\")\n",
        "#     )\n",
        "\n",
        "#     create_benchmark_summary_chart(benchmark_results, report_dir)\n",
        "\n",
        "#     # Generate comprehensive summary report\n",
        "#     summary_report = {\n",
        "#         \"analysis_timestamp\": platform.platform(),\n",
        "#         \"architecture\": detected_arch,\n",
        "#         \"composite_weights\": profile_info['weights'],\n",
        "#         \"best_single_algorithm\": {\n",
        "#             \"name\": \"Constant O(1)\",\n",
        "#             \"composite_score\": result_constant[\"COMPOSITE_SCORE\"],\n",
        "#             \"grade\": result_constant[\"SCORE_GRADE\"]\n",
        "#         },\n",
        "#         \"benchmark_summary\": benchmark_results[\"statistics\"],\n",
        "#         \"recommendations\": generate_recommendations(benchmark_results, profile_info)\n",
        "#     }\n",
        "\n",
        "#     with open(os.path.join(report_dir, \"comprehensive_summary.json\"), \"w\", encoding='utf-8') as f:\n",
        "#         json.dump(summary_report, f, indent=4)\n",
        "\n",
        "#     print(f\"\\n[Complete] Enhanced reports saved to '{report_dir}' directory\")\n",
        "#     print(f\"[Complete] Generated files:\")\n",
        "#     print(f\"  - Individual algorithm assessments (CSV)\")\n",
        "#     print(f\"  - Detailed comparison analysis (JSON)\")\n",
        "#     print(f\"  - Benchmark suite results (JSON)\")\n",
        "#     print(f\"  - Enhanced comparison charts (PNG)\")\n",
        "#     print(f\"  - Benchmark summary visualization (PNG)\")\n",
        "#     print(f\"  - Comprehensive summary report (JSON)\")\n"
      ],
      "metadata": {
        "id": "MJ0VhxpWN0D3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data generation"
      ],
      "metadata": {
        "id": "r786tZeynnjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Loaded {len(ba.algorithms_collection)} algorithms:\\n{list(ba.algorithms_collection.keys())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72bR36upoqLZ",
        "outputId": "77cad56f-fd11-4f67-86ba-4fabcdbfa40f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 15 algorithms:\n",
            "['Constant_O(1)_Formula', 'Logarithmic_O(log_n)_BinarySearch', 'Sqrt_O(sqrt_n)_PrimalityTest', 'Linear_O(n)_Sum', 'Linear_O(n)_ListAppend', 'Linear_O(n)_StringConcat', 'Linear_O(n)_DictCreation', 'Linear_O(n)_FactorialIter', 'Linear_O(n)_RecursivePower', 'N_Log_N_O(n_log_n)_Sort', 'Quadratic_O(n^2)_NestedLoops', 'Quadratic_O(n^2)_ListSearch', 'Cubic_O(n^3)_TripleLoops', 'Exponential_O(2^n)_Fibonacci', 'Factorial_O(n!)_Permutations']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BoE7JK5fowY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main body: ANALYZER CONFIGURATION AND ENVIRONMENT"
      ],
      "metadata": {
        "id": "jIPjZrdSpOLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\"*2 + '#'*150 + \"\\n\")\n",
        "print(\"\\t\"*2 + \"\\033[1mANALYZER CONFIGURATION AND ENVIRONMENT\\033[0m\")\n",
        "\n",
        "# 1. Perform the necessary initializations (no changes here)\n",
        "detected_arch = platform.machine().lower()\n",
        "analyzer = EnhancedCostAnalyzer(arch=detected_arch, profile=\"RESEARCH\")\n",
        "profile_info = analyzer.composite_calculator.get_profile_info()\n",
        "\n",
        "config_data = {\n",
        "    \"Detected Architecture\": detected_arch,\n",
        "    \"Available Profiles\": \", \".join(list(csc.PROFILE_WEIGHTS.keys())),\n",
        "    \"Selected Profile\": profile_info['profile'],\n",
        "    \"Profile Description\": profile_info['description'],\n",
        "    \"Profile Weights\": str(profile_info['weights'])\n",
        "}\n",
        "\n",
        "# 3. convert the dictionary items into a list of [key, value] pairs.\n",
        "config_df = pd.DataFrame(\n",
        "    list(config_data.items()),\n",
        "    columns=['Configuration Parameter', 'Value']\n",
        ")\n",
        "\n",
        "# 4. Setting the parameter name as the index makes the table look cleaner.\n",
        "config_df.set_index('Configuration Parameter', inplace=True)\n",
        "display(config_df)\n",
        "\n",
        "########################################################################\n",
        "# --- Enhanced Analysis using Benchmark Suite for proper calibration ---\n",
        "print(\"\\n\"*2 + '#'*150 + \"\\n\")\n",
        "print(\"\\t\"*8 + \"\\033[1mENHANCED ALGORITHM ANALYSIS: FULL SUITE SUMMARY\\033[0m\")\n",
        "\n",
        "\n",
        "# 1. Run the entire suite using the 'benchmark_suite' method.\n",
        "# This method first collects all raw results AND THEN updates the\n",
        "# reference values within the analyzer object before calculating scores.\n",
        "#\n",
        "# IMPORTANT: The results inside benchmark_results are calculated AFTER\n",
        "# the reference values have been calibrated on the suite itself.\n",
        "algorithms_for_benchmark = list(ba.algorithms_collection.items())\n",
        "benchmark_results = analyzer.benchmark_suite(algorithms_for_benchmark)\n",
        "\n",
        "# 2. Extract the results dictionary and convert to a DataFrame\n",
        "# The actual results are under the 'results' key.\n",
        "results_data = list(benchmark_results['results'].values())\n",
        "for i, name in enumerate(benchmark_results['results'].keys()):\n",
        "    results_data[i]['Algorithm'] = name\n",
        "\n",
        "results_df = pd.DataFrame(results_data)\n",
        "\n",
        "\n",
        "# 3. Prepare the DataFrame for display (same as before)\n",
        "display_columns = [\n",
        "    'Algorithm',\n",
        "    'CU',\n",
        "    'EU',\n",
        "    'CO2',\n",
        "    '$',\n",
        "    'CU_normalized',\n",
        "    'EU_normalized',\n",
        "    'CO2_normalized',\n",
        "    '$_normalized',\n",
        "    'SCORE_GRADE',\n",
        "    'EFFICIENCY_RATING',\n",
        "    'COMPOSITE_SCORE'\n",
        "]\n",
        "# Ensure all columns exist before trying to display them\n",
        "# This handles cases where some results might not be generated\n",
        "existing_display_columns = [col for col in display_columns if col in results_df.columns]\n",
        "results_df = results_df[existing_display_columns]\n",
        "\n",
        "# Sort the table by the composite score in descending order\n",
        "results_df = results_df.sort_values(by='COMPOSITE_SCORE', ascending=False)\n",
        "\n",
        "# Set the 'Algorithm' column as the table index\n",
        "results_df.set_index('Algorithm', inplace=True)\n",
        "\n",
        "# Set display options for float numbers\n",
        "pd.options.display.float_format = '{:,.6f}'.format\n",
        "\n",
        "# 4. Print the final, formatted table\n",
        "display(results_df)\n",
        "\n",
        "########################################################################\n",
        "# --- Detailed Comparison of All Algorithms Against the Best Performer ---\n",
        "\n",
        "print(\"\\n\"*2 + '#'*150 + \"\\n\")\n",
        "print(\"\\t\"*4 + \"\\033[1mDETAILED COMPARISON AGAINST THE BEST PERFORMING ALGORITHM\\033[0m\")\n",
        "\n",
        "# 1. Identify the best algorithm to use as our baseline for comparison.\n",
        "# We get this information from the statistics calculated by the benchmark_suite.\n",
        "baseline_name = benchmark_results['statistics']['best_algorithm']\n",
        "baseline_metrics = benchmark_results['results'][baseline_name]\n",
        "\n",
        "print(f\"\\n[Comparison] Baseline Algorithm (Highest Score): '{baseline_name}'\")\n",
        "\n",
        "# 2. Prepare a list to hold the comparison data for each algorithm.\n",
        "comparison_data = []\n",
        "\n",
        "# Iterate through all algorithm results to compare them against the baseline.\n",
        "for name, current_metrics in benchmark_results['results'].items():\n",
        "    # Calculate the difference in composite score.\n",
        "    score_diff = current_metrics['COMPOSITE_SCORE'] - baseline_metrics['COMPOSITE_SCORE']\n",
        "\n",
        "    # Calculate the percentage change for each raw metric.\n",
        "    # Formula: ((current - baseline) / baseline) * 100\n",
        "    # A positive value means it's \"more expensive\" or \"worse\" than the baseline.\n",
        "    comparison_row = {\n",
        "        'Algorithm': name,\n",
        "        'COMPOSITE_SCORE': current_metrics['COMPOSITE_SCORE'],\n",
        "        'Score_vs_Best': score_diff, # Will be 0 for the best, negative for others.\n",
        "    }\n",
        "\n",
        "    for metric in [\"CU\", \"EU\", \"CO2\", \"$\"]:\n",
        "        current_val = current_metrics[metric]\n",
        "        baseline_val = baseline_metrics[metric]\n",
        "\n",
        "        # Avoid division by zero, though unlikely with this data.\n",
        "        if baseline_val > 0:\n",
        "            percent_change = ((current_val - baseline_val) / baseline_val) * 100\n",
        "        else:\n",
        "            percent_change = float('inf')\n",
        "\n",
        "        comparison_row[f'{metric}_%_vs_Best'] = percent_change\n",
        "\n",
        "    comparison_data.append(comparison_row)\n",
        "\n",
        "# 3. Create the comparison DataFrame.\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df.sort_values(by='COMPOSITE_SCORE', ascending=False, inplace=True)\n",
        "comparison_df.set_index('Algorithm', inplace=True)\n",
        "\n",
        "# 4. Format the DataFrame for better readability using the .style attribute.\n",
        "# This gives us more control over formatting individual columns.\n",
        "formatted_comparison = comparison_df.style.format({\n",
        "    'COMPOSITE_SCORE': '{:.2f}',\n",
        "    'Score_vs_Best': '{:+.2f}', # Add a '+' sign for positive numbers (only the baseline will be 0)\n",
        "    'CU_%_vs_Best': '{:+.2f}%',\n",
        "    'EU_%_vs_Best': '{:+.2f}%',\n",
        "    'CO2_%_vs_Best': '{:+.2f}%',\n",
        "    '$_%_vs_Best': '{:+.2f}%',\n",
        "}).background_gradient(\n",
        "    cmap='Reds', # 'Reds' cmap will highlight larger (worse) percentages in red\n",
        "    subset=['CU_%_vs_Best', 'EU_%_vs_Best', 'CO2_%_vs_Best', '$_%_vs_Best']\n",
        ").bar(\n",
        "    subset=['Score_vs_Best'], # Add bars to show the score difference visually\n",
        "    align='zero',\n",
        "    color=['#d65f5f', '#5fba7d'] # Red for negative, green for positive\n",
        ")\n",
        "\n",
        "# Display the final, styled table.\n",
        "display(formatted_comparison)\n",
        "\n",
        "########################################################################\n",
        "# --- Final Summaries: Statistics, Profile Comparison, and Environment ---\n",
        "\n",
        "print(\"\\n\"*2 + '#'*150 + \"\\n\")\n",
        "print(\"\\t\"*5 + \"\\033[1mFINAL SUMMARIES AND CONTEXTUAL ANALYSIS\\033[0m\")\n",
        "\n",
        "# --- Block 1: Benchmark Suite Statistics ---\n",
        "print(\"\\n[Benchmark] Statistical Summary:\")\n",
        "\n",
        "# The statistics are already in a dictionary, perfect for a key-value table.\n",
        "stats_data = benchmark_results['statistics']\n",
        "stats_df = pd.DataFrame(\n",
        "    list(stats_data.items()),\n",
        "    columns=['Statistic', 'Value']\n",
        ")\n",
        "stats_df.set_index('Statistic', inplace=True)\n",
        "\n",
        "# Format float values to 2 decimal places for readability\n",
        "stats_df['Value'] = stats_df['Value'].apply(\n",
        "    lambda x: f\"{x:.2f}\" if isinstance(x, (int, float)) else x\n",
        ")\n",
        "display(stats_df)\n",
        "\n",
        "\n",
        "# --- Block 2: Profile Comparison Analysis ---\n",
        "print(f\"\\n[Profile Comparison] How a single algorithm's score changes based on the active profile: {detected_arch}\")\n",
        "data = []\n",
        "for metric, refs in benchmark_results['updated_references'].items():\n",
        "    data.append({\n",
        "        'metric': metric,\n",
        "        'min': f\"{refs['min']:g}\",\n",
        "        'max': f\"{refs['max']:g}\",\n",
        "        'typical': f\"{refs['typical']:g}\"\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "display(df)\n",
        "\n",
        "# We'll use a simple, consistent algorithm for this \"what-if\" analysis.\n",
        "test_algorithm_func = ba.algorithms_collection['Constant_O(1)_Formula']\n",
        "\n",
        "print(f\"\\n[Info] Using '{list(ba.algorithms_collection.keys())[0]}' as the test case (current environment).\")\n",
        "\n",
        "profile_comparison_results = []\n",
        "\n",
        "# Iterate through all available profiles defined in PROFILE_WEIGHTS.\n",
        "for profile_name in csc.PROFILE_WEIGHTS.keys():\n",
        "\n",
        "    # 1. Create a temporary, clean analyzer for each profile.\n",
        "    temp_analyzer = EnhancedCostAnalyzer(arch=detected_arch, profile=profile_name)\n",
        "\n",
        "    # 3. Now, analyze our single test function using the fully configured and calibrated analyzer.\n",
        "    # The score will now be calculated relative to the *calibrated* min/max for that profile's logic.\n",
        "    test_result = temp_analyzer.analyze_function(test_algorithm_func)\n",
        "\n",
        "    profile_comparison_results.append({\n",
        "        'Profile': profile_name,\n",
        "        'Composite Score': test_result['COMPOSITE_SCORE'],\n",
        "        'Grade': test_result['SCORE_GRADE']\n",
        "    })\n",
        "\n",
        "# Create and display the DataFrame for the profile comparison.\n",
        "profile_df = pd.DataFrame(profile_comparison_results)\n",
        "profile_df.sort_values(by='Composite Score', ascending=False, inplace=True)\n",
        "profile_df.set_index('Profile', inplace=True)\n",
        "\n",
        "# Display the final table.\n",
        "display(profile_df.style.format({'Composite Score': '{:.2f}'}))\n",
        "\n",
        "\n",
        "\n",
        "# --- Block 3: Environmental Impact Analysis ---\n",
        "# Fetch the carbon intensity value.\n",
        "carbon_intensity = analyzer.fetch_carbon_intensity()\n",
        "print(f\"\\n[Environment] Current Carbon Intensity: {carbon_intensity:g} kgCO2/kWh\")\n",
        "\n"
      ],
      "metadata": {
        "id": "AMAqOBg9sNgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main body: REPORTING AND SAVING RESULTS"
      ],
      "metadata": {
        "id": "ZzVrw5HngZT_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z8begp1Y4m8S"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVING_FLAG = True\n",
        "DEFAULT_REPORT_DIR = \"./enhanced_reports\"\n",
        "EXT_PATH = DEFAULT_REPORT_DIR + '/enhanced_reports'\n",
        "\n",
        "# detected_arch = platform.machine().lower()\n",
        "# analyzer = EnhancedCostAnalyzer(arch=detected_arch, profile=\"RESEARCH\")\n",
        "# profile_info = analyzer.composite_calculator.get_profile_info()\n",
        "# algorithms_for_benchmark = list(ba.algorithms_collection.items())\n",
        "# benchmark_results = analyzer.benchmark_suite(algorithms_for_benchmark)\n",
        "\n",
        "# results_data = list(benchmark_results['results'].values())\n",
        "# for i, name in enumerate(benchmark_results['results'].keys()):\n",
        "#     results_data[i]['Algorithm'] = name\n",
        "\n",
        "# results_df = pd.DataFrame(results_data)\n",
        "# --- Reporting and Saving ---\n",
        "\n",
        "print(\"\\n\"*2 + '#'*150 + \"\\n\")\n",
        "print(\"\\t\"*8 + \"\\033[1mREPORTING AND SAVING RESULTS\\033[0m\")\n",
        "\n",
        "# --- 1. Generate All Data and Visualizations (always happens) ---\n",
        "\n",
        "# a) Generate the summary chart for all algorithms\n",
        "print(\"\\n[Chart] Generating overall benchmark summary chart...\")\n",
        "u.create_benchmark_summary_chart(benchmark_results, DEFAULT_REPORT_DIR)\n",
        "print(\"  > Chart is displayed above.\")\n",
        "\n",
        "# --- Generate Comparison Charts for All Algorithms ---\n",
        "print(\"\\n[Chart] Generating comparison charts for each algorithm vs. the best...\")\n",
        "\n",
        "# 1. Identify the baseline algorithm (the best one).\n",
        "best_algo_name = benchmark_results['statistics']['best_algorithm']\n",
        "best_algo_results = benchmark_results['results'][best_algo_name]\n",
        "print(f\"  > Baseline for comparison is '{best_algo_name}'.\")\n",
        "\n",
        "# 2. IMPORTANT: Create the main reports directory BEFORE the loop.\n",
        "# The REPORT_DIR variable comes from the main script context (e.g., \"enhanced_reports\").\n",
        "os.makedirs(DEFAULT_REPORT_DIR, exist_ok=True)\n",
        "\n",
        "# 3. Loop through all algorithms and generate a chart for each one.\n",
        "for current_algo_name, current_algo_results in benchmark_results['results'].items():\n",
        "\n",
        "    if current_algo_name == best_algo_name:\n",
        "        continue\n",
        "\n",
        "    safe_current_name = u.make_safe_filename(current_algo_name)\n",
        "    safe_best_name = u.make_safe_filename(best_algo_name)\n",
        "    chart_filename = f\"comparison_{safe_current_name}_vs_{safe_best_name}.png\"\n",
        "\n",
        "    # Construct the full, final path for the image file.\n",
        "    output_filepath = os.path.join(DEFAULT_REPORT_DIR, chart_filename)\n",
        "\n",
        "    print(f\"  > Generating chart: {chart_filename}\")\n",
        "\n",
        "    # Call the updated chart function with the full file path.\n",
        "    u.create_enhanced_comparison_chart(\n",
        "        result1=best_algo_results,\n",
        "        result2=current_algo_results,\n",
        "        output_filepath=output_filepath, # Pass the full path\n",
        "        names=(best_algo_name, current_algo_name)\n",
        "    )\n",
        "\n",
        "print(\"  > All comparison charts have been generated and displayed.\")\n",
        "\n",
        "# --- Generate and Display Comprehensive Summary Report ---\n",
        "\n",
        "print(\"\\n\" + \"=\"*110)\n",
        "print(\"\\t\"*5 + \"\\033[1mCOMPREHENSIVE SUMMARY REPORT\\033[0m\")\n",
        "print(\"=\"*110)\n",
        "\n",
        "# 1. Prepare all the data components (no change here)\n",
        "best_algo_name = benchmark_results['statistics']['best_algorithm']\n",
        "worst_algo_name = benchmark_results['statistics']['worst_algorithm']\n",
        "best_algo_results = benchmark_results['results'][best_algo_name]\n",
        "worst_algo_results = benchmark_results['results'][worst_algo_name]\n",
        "\n",
        "# 2. Create the summary_report dictionary as before\n",
        "summary_report = {\n",
        "    \"analysis_timestamp\": platform.platform(),\n",
        "    \"architecture\": detected_arch,\n",
        "    \"profile_used\": profile_info,\n",
        "    \"benchmark_summary_stats\": benchmark_results[\"statistics\"],\n",
        "    \"best_algorithm_details\": best_algo_results,\n",
        "    \"worst_algorithm_details\": worst_algo_results,\n",
        "    \"recommendations\": u.generate_recommendations(benchmark_results, profile_info)\n",
        "}\n",
        "\n",
        "# 3. Flatten the complex dictionary into a list of [key, value] pairs for the DataFrame\n",
        "report_data_list = []\n",
        "\n",
        "# a) Add top-level info\n",
        "report_data_list.append(['Analysis Timestamp', summary_report['analysis_timestamp']])\n",
        "report_data_list.append(['Architecture', summary_report['architecture']])\n",
        "\n",
        "# b) Unroll the 'profile_used' dictionary\n",
        "for key, value in summary_report['profile_used'].items():\n",
        "    # Make keys more descriptive, e.g., \"Profile - Description\"\n",
        "    descriptive_key = f\"Profile - {key.replace('_', ' ').capitalize()}\"\n",
        "    report_data_list.append([descriptive_key, str(value)])\n",
        "\n",
        "# c) Unroll the 'benchmark_summary_stats' dictionary\n",
        "for key, value in summary_report['benchmark_summary_stats'].items():\n",
        "    descriptive_key = f\"Stat - {key.replace('_', ' ').title()}\"\n",
        "    # Format numbers for better display\n",
        "    formatted_value = f\"{value:.2f}\" if isinstance(value, float) else value\n",
        "    report_data_list.append([descriptive_key, formatted_value])\n",
        "\n",
        "# d) Unroll the 'recommendations' list\n",
        "for i, rec_text in enumerate(summary_report['recommendations']):\n",
        "    report_data_list.append([f'Recommendation #{i+1}', rec_text])\n",
        "\n",
        "# e) Unroll details for the BEST algorithm\n",
        "for key, value in summary_report['best_algorithm_details'].items():\n",
        "    if key == 'Algorithm': continue # Skip redundant name\n",
        "    descriptive_key = f\"Best Algo ({best_algo_name}) - {key}\"\n",
        "    formatted_value = f\"{value:g}\" if isinstance(value, float) else value\n",
        "    report_data_list.append([descriptive_key, formatted_value])\n",
        "\n",
        "# f) Unroll details for the WORST algorithm\n",
        "for key, value in summary_report['worst_algorithm_details'].items():\n",
        "    if key == 'Algorithm': continue # Skip redundant name\n",
        "    descriptive_key = f\"Worst Algo ({worst_algo_name}) - {key}\"\n",
        "    formatted_value = f\"{value:g}\" if isinstance(value, float) else value\n",
        "    report_data_list.append([descriptive_key, formatted_value])\n",
        "\n",
        "\n",
        "# 4. Create and display the DataFrame\n",
        "summary_df = pd.DataFrame(report_data_list, columns=['Metric', 'Value'])\n",
        "summary_df.set_index('Metric', inplace=True)\n",
        "display(summary_df)\n",
        "\n",
        "\n",
        "# --- 2. Handle File Saving Operations based on SAVING_FLAG ---\n",
        "\n",
        "if not SAVING_FLAG:\n",
        "    # If not saving, just print a confirmation that visuals were displayed.\n",
        "    print(\"\\n[Info] SAVING_FLAG is set to False. All results and charts are displayed above but not saved to disk.\")\n",
        "\n",
        "else:\n",
        "    # If saving is enabled, proceed with all file I/O operations.\n",
        "    print(f\"\\n[Info] SAVING_FLAG is True. Saving all reports to directory: '{DEFAULT_REPORT_DIR}'\")\n",
        "\n",
        "    # The directory is already created by the charting functions, so we just confirm.\n",
        "    os.makedirs(DEFAULT_REPORT_DIR, exist_ok=True)\n",
        "\n",
        "    # a) Save main DataFrames as CSV files\n",
        "    results_df.to_csv(os.path.join(DEFAULT_REPORT_DIR, \"01_full_benchmark_summary.csv\"))\n",
        "    comparison_df.to_csv(os.path.join(DEFAULT_REPORT_DIR, \"02_comparison_vs_best.csv\"))\n",
        "    print(\"  > Saved main DataFrames to CSV.\")\n",
        "\n",
        "    # b) Save raw results dictionaries as JSON files\n",
        "    with open(os.path.join(DEFAULT_REPORT_DIR, \"03_benchmark_suite_raw_data.json\"), \"w\", encoding='utf-8') as f:\n",
        "        json.dump(benchmark_results, f, indent=4)\n",
        "    print(\"  > Saved raw benchmark data to JSON.\")\n",
        "\n",
        "    # c) Save the comprehensive summary report\n",
        "    print(\"\\n[Info] Saving comprehensive summary report to JSON file...\")\n",
        "    try:\n",
        "        with open(os.path.join(DEFAULT_REPORT_DIR, \"comprehensive_summary_report.json\"), \"w\", encoding='utf-8') as f:\n",
        "            json.dump(summary_report, f, indent=4, ensure_ascii=False)\n",
        "        print(\"  > Report saved successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  > Error saving summary report: {e}\")\n",
        "\n",
        "    # d) Create a ZIP archive of all generated reports\n",
        "    print(\"  > Creating ZIP archive of all reports...\")\n",
        "    try:\n",
        "        archive_path = shutil.make_archive('enhanced_reports', 'gztar', root_dir=DEFAULT_REPORT_DIR)\n",
        "        print(f\"  > Successfully created archive: '{archive_path}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"  > Error creating archive: {e}\")\n",
        "\n",
        "# --- Final Confirmation Message ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\033[1mANALYSIS AND REPORTING COMPLETE\\033[0m\")\n",
        "if SAVING_FLAG:\n",
        "    print(f\"All reports have been saved in the '{DEFAULT_REPORT_DIR}' directory and compressed into '{archive_path}'.\")\n",
        "else:\n",
        "    print(\"Analysis results were displayed above. No files were saved.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "PGKGz7WiITkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main body: ANALYSIS OF EXTERNAL SOURCE FILES"
      ],
      "metadata": {
        "id": "gkGVzYWkgjdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "DEFAULT_EXAMPLES_PATH = \"./examples\"\n",
        "\n",
        "print(\"\\n\"*2 + '#'*150 + \"\\n\")\n",
        "print(\"\\t\"*5 + \"\\033[1mANALYSIS OF EXTERNAL SOURCE FILES (LLVM, PTX, Python)\\033[0m\")\n",
        "\n",
        "detected_arch = platform.machine().lower()\n",
        "analyzer = EnhancedCostAnalyzer(arch=detected_arch, profile=\"RESEARCH\")\n",
        "profile_info = analyzer.composite_calculator.get_profile_info()\n",
        "\n",
        "if not os.path.isdir(DEFAULT_EXAMPLES_PATH):\n",
        "    print(f\"\\n[Info] Directory '{DEFAULT_EXAMPLES_PATH}' not found. Skipping analysis of external files.\")\n",
        "else:\n",
        "    # --- Step 1: Collect all raw results from all files and functions ---\n",
        "    all_individual_results = []\n",
        "\n",
        "    # Define the file types we want to analyze and the function to use for each.\n",
        "    file_types_to_analyze = {\n",
        "        'Python': {\n",
        "            'extension': 'py',\n",
        "            'analysis_func': analyzer.analyze_py_file # This must be defined in your analyzer class\n",
        "        },\n",
        "        'LLVM IR': {\n",
        "            'extension': 'll',\n",
        "            'analysis_func': analyzer.analyze_llvm_ir\n",
        "        },\n",
        "        'PTX GPU': {\n",
        "            'extension': 'ptx',\n",
        "            'analysis_func': analyzer.analyze_ptx\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(f\"\\n[Info] Found directory '{DEFAULT_EXAMPLES_PATH}'. Searching for compatible files...\")\n",
        "\n",
        "    # Loop through our defined file types and find all matching files.\n",
        "    for file_type, config in file_types_to_analyze.items():\n",
        "        search_pattern = os.path.join(DEFAULT_EXAMPLES_PATH, f\"**/*.{config['extension']}\")\n",
        "        found_files = glob.glob(search_pattern, recursive=True)\n",
        "\n",
        "        for file_path in found_files:\n",
        "            print(f\"  > Analyzing {file_type} file: {file_path}\")\n",
        "\n",
        "            # Call the appropriate analysis function\n",
        "            results_from_file = config['analysis_func'](file_path)\n",
        "\n",
        "            # The result can be a list (for .py) or a single dict (for others)\n",
        "            # To handle both consistently, we'll ensure we have a list.\n",
        "            if not isinstance(results_from_file, list):\n",
        "                # For non-python files, wrap the single result dict in a list\n",
        "                single_result = results_from_file\n",
        "                single_result['Source File'] = os.path.basename(file_path)\n",
        "                single_result['Function Name'] = '-' # No specific function name\n",
        "                single_result['File Type'] = file_type\n",
        "                results_from_file = [single_result]\n",
        "\n",
        "            # Add the File Type to results from Python files\n",
        "            if file_type == 'Python':\n",
        "                for res in results_from_file:\n",
        "                    res['File Type'] = file_type\n",
        "\n",
        "            # Use extend to add all items from the list to our main results\n",
        "            all_individual_results.extend(results_from_file)\n",
        "\n",
        "    # --- Step 2: Aggregate the results by source file ---\n",
        "    aggregated_results = {}\n",
        "\n",
        "    for result in all_individual_results:\n",
        "        file_name = result['Source File']\n",
        "\n",
        "        if file_name not in aggregated_results:\n",
        "            # Initialize the entry for this file\n",
        "            aggregated_results[file_name] = {\n",
        "                'Source File': file_name,\n",
        "                'File Type': result.get('File Type', 'Unknown'),\n",
        "                'Function Name': [], # We will collect function names in a list\n",
        "                'CU': 0, 'EU': 0, 'CO2': 0, '$': 0\n",
        "            }\n",
        "\n",
        "        # Aggregate raw numeric metrics\n",
        "        for metric in ['CU', 'EU', 'CO2', '$']:\n",
        "            aggregated_results[file_name][metric] += result.get(metric, 0)\n",
        "\n",
        "        # Collect function names, avoiding duplicates and placeholders\n",
        "        func_name = result.get('Function Name')\n",
        "        if func_name and func_name not in aggregated_results[file_name]['Function Name']:\n",
        "            aggregated_results[file_name]['Function Name'].append(func_name)\n",
        "\n",
        "    # --- Step 3: Finalize the aggregated data for display ---\n",
        "    final_aggregated_list = []\n",
        "    for file_name, data in aggregated_results.items():\n",
        "        # Join the list of function names into a single readable string\n",
        "        if data['Function Name']:\n",
        "            data['Function Name'] = ', '.join(sorted(data['Function Name']))\n",
        "        else:\n",
        "            # Handle files like .ll or .ptx that have no function names\n",
        "            data['Function Name'] = '-'\n",
        "\n",
        "        # If the file had an error or no functions, its raw metrics will be 0.\n",
        "        # We need to handle this to avoid division by zero in score calculation.\n",
        "        if data['CU'] > 0:\n",
        "            # Recalculate the composite score based on the SUMMED raw metrics\n",
        "            final_data_with_score = analyzer.composite_calculator.calculate_composite_score(data)\n",
        "            final_aggregated_list.append(final_data_with_score)\n",
        "        else:\n",
        "            # For files with no analyzable content, just add them with 0 scores\n",
        "            data['COMPOSITE_SCORE'] = data.get('COMPOSITE_SCORE', 0)\n",
        "            data['SCORE_GRADE'] = data.get('SCORE_GRADE', 'N/A')\n",
        "            final_aggregated_list.append(data)\n",
        "\n",
        "    # --- Step 4: Create and display the final aggregated DataFrame ---\n",
        "    if not final_aggregated_list:\n",
        "        print(\"\\n[Info] No compatible files (.py, .ll, .ptx) found in the 'examples' directory.\")\n",
        "    else:\n",
        "        external_df = pd.DataFrame(final_aggregated_list)\n",
        "\n",
        "        # Define the columns for the final output table\n",
        "        display_columns = [\n",
        "            'Source File', 'File Type', 'Function Name',\n",
        "            'COMPOSITE_SCORE', 'SCORE_GRADE', 'CU', 'EU', 'CO2', '$'\n",
        "        ]\n",
        "        # Ensure we only try to access columns that actually exist\n",
        "        existing_cols = [col for col in display_columns if col in external_df.columns]\n",
        "        external_df = external_df[existing_cols]\n",
        "\n",
        "        # Sort the results and set the index for a clean look\n",
        "        external_df.sort_values(by='COMPOSITE_SCORE', ascending=False, inplace=True)\n",
        "        external_df.set_index('Source File', inplace=True)\n",
        "\n",
        "        print(\"\\n[Analysis] Aggregated Assessment of External Files:\")\n",
        "        display(external_df)"
      ],
      "metadata": {
        "id": "846xfBT5b61l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting\n",
        "if not external_df.empty:\n",
        "    print(\"\\n[Chart] Generating custom performance scatter plot...\")\n",
        "\n",
        "    scatterplot_filepath = os.path.join(u.DEFAULT_REPORT_DIR, \"file_performance_scatterplot.png\")\n",
        "\n",
        "    # Call the new function. You can change the y-axis metric here if you want.\n",
        "    # For example: y_axis_metric='EU'\n",
        "    u.create_file_performance_scatterplot(\n",
        "        data=external_df,\n",
        "        output_filepath=scatterplot_filepath,\n",
        "        y_axis_metric='CU' # This can be changed to 'EU', 'CO2', or '$'\n",
        "    )\n",
        ""
      ],
      "metadata": {
        "id": "tSHsmACOL9pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main body: ANALYSIS OF EXTERNAL REPO (only ONE)"
      ],
      "metadata": {
        "id": "_BfiSRHcsMtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import platform\n",
        "\n",
        "import utils as u\n",
        "\n",
        "print(\"\\n\"*2 + '#'*150 + \"\\n\")\n",
        "print(\"\\t\"*6 + \"\\033[1mCOMPREHENSIVE REPOSITORY ANALYSIS\\033[0m\")\n",
        "\n",
        "REPO_URL = \"https://github.com/s-kav/ds_tools.git\"\n",
        "LOCAL_REPO_PATH = \"ds_tools\"\n",
        "detected_arch = platform.machine().lower()\n",
        "\n",
        "if not os.path.isdir(LOCAL_REPO_PATH):\n",
        "    print(f\"[Info] Repository '{LOCAL_REPO_PATH}' not found locally. Cloning from {REPO_URL}...\")\n",
        "    !git clone {REPO_URL}\n",
        "    print(\"[Info] Cloning complete.\")\n",
        "else:\n",
        "    print(f\"[Info] Repository '{LOCAL_REPO_PATH}' already exists locally.\")\n",
        "\n",
        "# The 'detected_arch' variable must be defined from your first initialization block\n",
        "repository_df = u.analyze_repository(\n",
        "    repo_path=LOCAL_REPO_PATH,\n",
        "    detected_arch=detected_arch, # Pass the architecture here\n",
        "    verbose=False\n",
        ")\n",
        "repository_df.set_index('PROFILE NAME', inplace=True)\n",
        "\n",
        "if not repository_df.empty:\n",
        "    print(f\"\\n[Analysis] Aggregated assessment for repository: '{LOCAL_REPO_PATH}'\")\n",
        "    display(repository_df.style.format({\n",
        "        'COMPOSITE_SCORE': '{:.2f}',\n",
        "        'CU': '{:,.0f}',\n",
        "        'EU': '{:,.4f}',\n",
        "        'CO2': '{:,.4f}',\n",
        "        '$': '{:,.4f}',\n",
        "    }))"
      ],
      "metadata": {
        "id": "EYrUXB_XMM2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting\n",
        "if not repository_df.empty:\n",
        "    print(\"\\n[Chart] Generating custom performance scatter plot...\")\n",
        "\n",
        "    scatterplot_filepath = os.path.join(u.DEFAULT_REPORT_DIR, \"repository_performance_scatterplot.png\")\n",
        "\n",
        "    # Call the new function. You can change the y-axis metric here if you want.\n",
        "    # For example: y_axis_metric='EU'\n",
        "    u.create_file_performance_scatterplot(\n",
        "        data=repository_df,\n",
        "        output_filepath=scatterplot_filepath,\n",
        "        y_axis_metric='CU' # This can be changed to 'EU', 'CO2', or '$'\n",
        "    )"
      ],
      "metadata": {
        "id": "w6pge3Kon7tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main body: ANALYSIS OF EXTERNAL REPOS"
      ],
      "metadata": {
        "id": "cTzFcpyAvpEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "import utils as u\n",
        "\n",
        "print(\"\\n\"*2 + '#'*150 + \"\\n\")\n",
        "print(\"\\t\"*6 + \"\\033[1mCOMPREHENSIVE REPOSITORY ANALYSIS\\033[0m\")\n",
        "\n",
        "# 1.  List of URL-addresses of  repos\n",
        "REPO_URLS = [\n",
        "    \"https://github.com/s-kav/ds_tools.git\",\n",
        "    \"https://github.com/s-kav/kz_data_imputation.git\",\n",
        "    \"https://github.com/s-kav/s3_s4_activation_function.git\",\n",
        "    \"https://github.com/egorsmkv/radtts-uk-vocos-demo.git\",\n",
        "    \"https://github.com/PINTO0309/yolov9_wholebody34_heatmap_vis.git\",\n",
        "    \"https://github.com/MaAI-Kyoto/MaAI.git\",\n",
        "    \"https://github.com/TheAlgorithms/Python.git\",\n",
        "    \"https://github.com/tweepy/tweepy.git\",\n",
        "    \"https://github.com/lincolnloop/python-qrcode.git\",\n",
        "    \"https://github.com/prompt-toolkit/python-prompt-toolkit.git\"\n",
        "]\n",
        "detected_arch = platform.machine().lower()\n",
        "all_repo_data = {}\n",
        "# Analyze each repository and store its data\n",
        "for repo_url in REPO_URLS:\n",
        "    local_repo_path = repo_url.split('/')[-1].replace('.git', '')\n",
        "\n",
        "    if not os.path.isdir(local_repo_path):\n",
        "        subprocess.run(['git', 'clone', repo_url], capture_output=True, text=True)\n",
        "\n",
        "    repository_df = u.analyze_repository(\n",
        "        repo_path=local_repo_path,\n",
        "        detected_arch=detected_arch,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    if not repository_df.empty:\n",
        "        repository_df.set_index('PROFILE NAME', inplace=True)\n",
        "        all_repo_data[local_repo_path] = repository_df # Store the result\n",
        "\n",
        "        print(f\"\\n[Analysis] Aggregated assessment for repository: '{local_repo_path}'\")\n",
        "        display(repository_df.style.format({\n",
        "            'COMPOSITE_SCORE': '{:.2f}',\n",
        "            'CU': '{:,.0f}',\n",
        "            'EU': '{:,.4f}',\n",
        "            'CO2': '{:,.4f}',\n",
        "            '$': '{:,.4f}',\n",
        "        }))"
      ],
      "metadata": {
        "id": "YbxxJ3bJvrgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REPO_URLS = [\n",
        "    \"https://github.com/s-kav/ds_tools.git\",\n",
        "    \"https://github.com/s-kav/kz_data_imputation.git\",\n",
        "    \"https://github.com/s-kav/s3_s4_activation_function.git\",\n",
        "    \"https://github.com/egorsmkv/radtts-uk-vocos-demo.git\",\n",
        "    \"https://github.com/PINTO0309/yolov9_wholebody34_heatmap_vis.git\",\n",
        "    \"https://github.com/MaAI-Kyoto/MaAI.git\",\n",
        "    \"https://github.com/TheAlgorithms/Python.git\",\n",
        "    \"https://github.com/tweepy/tweepy.git\",\n",
        "    \"https://github.com/lincolnloop/python-qrcode.git\"\n",
        "]\n",
        "for repo_url in REPO_URLS:\n",
        "  local_repo_path = repo_url.split('/')[-1].replace('.git', '')\n",
        "  shutil.rmtree('./' + local_repo_path)"
      ],
      "metadata": {
        "id": "6TxsYgSIvrkH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Plotting section: generates ONE summary chart after all analyses are done ---\n",
        "\n",
        "if all_repo_data:\n",
        "    print(\"\\n[Chart] Generating summary comparison chart for all repositories...\")\n",
        "\n",
        "    summary_chart_filepath = os.path.join(u.DEFAULT_REPORT_DIR, \"repository_comparison_summary.png\")\n",
        "\n",
        "    # Call the new function to create the comparison chart.\n",
        "    # You can change the profile to plot, e.g., \"RESEARCH\", \"COMMERCIAL\", etc.\n",
        "    u.create_repository_comparison_chart(\n",
        "        repo_data=all_repo_data,\n",
        "        output_filepath=summary_chart_filepath,\n",
        "        profile_to_plot=\"TOTAL\" # This determines which row to use from each DataFrame\n",
        "    )\n",
        "else:\n",
        "    print(\"\\n[Info] No data was collected from repositories, skipping chart generation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ9RhRgVA0-j",
        "outputId": "05e88c61-6259-4632-a3b0-4f59a84910f9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Chart] Generating summary comparison chart for all repositories...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# end"
      ],
      "metadata": {
        "id": "u7aS9LY9tLW3"
      }
    }
  ]
}